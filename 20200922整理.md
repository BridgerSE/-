## 整理

1，AUC（越详细越好）

​	 答：AUC是用来衡量模型的稳定性指标（或正负样本rank结果的优劣）

​			横坐标是fpr，纵坐标是tpr，曲线代表二者的函数关系，曲线下的面积为AUC

​			取值区间在[0.5, 1]，AUC小于0.5时取反即可。

​	*拓展问题：logloss与auc的关系？loss下降auc不下降时如何处理？*

​	AUC代码实现:

```python
def AUC(labels, preds):
    //边界条件暂时不考虑
    pos_index = [i for i in range(len(labels)) if labels[i] == 1.0]
    neg_index = [i for i in range(len(labels)) if labels[i] == 0.0]
    
    if len(pos_index) == 0 || len(neg_index) == 0:
        return 0.5
    
    score = 0.0
    for i in pos_index:
        for j in neg_index:
            if preds[i] > preds[j]:
                score += 1
            else score += 0.5
        
    return score/(len(pos_index) *len(neg_index))
	//复杂度为O(n)~O(n*n/4)
    
```

2，TPR/FPR， P/R

​	 答：1，TPR：真阳率(true positive rate), TPR = TP /  ( TP + FN )

​					分类器/模型**正确预测的正样本**占**真实正样本**的比例

​			  2，FPR：假阳率(false positive rate), FPR = FP / (TN + FP)

​					分类器/模型**错误预测的负样本**占**真实负样本**的比例

​			  3，Precision：准确率，P = TP /（ TP + FP ）

​					 **正确预测的正样本**占**预测为正样本**的比例

​			  4， Recall：召回率， R = TP /  (TP + FN)

​					**正确预测的正样本**占**真实正样本**的比例（至于PR的区别）

​			*拓展问题：PR的矛盾性存在于何处？*

3，L1/L2正则(越详细越好)

​			 1、分布：L1为拉普拉斯分布， L2为高斯分布

​			 2、W参数衰减：L1为sgn(W)相关， L2为W相关（推导loss可得）

​					影响：W偏大时，L2的W衰减速度大于L1

​								W偏小时，L1得W衰减速度大于L2

​			 3、正则体现在：W的衰减，极端情况下假设所有神经元的W都衰减接近0，那么每个神经元的影响将会被降到至最低。

​			 4、L1可导吗？ 不可倒，w=0处的左右导数不等，0处导数值可指定为0.5或±1

4，FM模型：(n为特征数量，k为embedding_dim)

​			1、原始公式，参数复杂度O(n *n) ，运算复杂度O(n *n)

​			2、原始的因式分解公式，参数复杂度O(n *k)，运算复杂度O(n *n *k)

​			3、完全平方公式优化后，参数复杂度O(n *k)，运算复杂度O(n *k)

​			4、FM优点/缺点：embedding_dim不会太大，太大容易不收敛

​			*拓展问题1：FM为什么能缓解稀疏场景下的过拟合？*

​			*拓展问题2：FFM公式，复杂度相关*

5，Transformer结构(attention相关)：

​			1、Transformer的3个attention结构（encoder一个，decoder两个，还有LN，各个节点的QKV要理解清楚）

​			2、attention公式：Attention = *softmax*(Q*K.T/sqrt(dk)) V

​				 self-attention中，Q = Wq X， K = Wk X， V = Wv X

​				 *拓展问题：self-attention的实现是否可以简化？sqrt(dk)可否用其他操作代替*？

​			3、Transformer的mask以及相关数据处理方式：

​				  padding_mask: padding短序列，softmax之前的结果需要进行处理，padding时填0的的位置需要用一个近似于负无穷的值取代（因为e^(-inf) = 0），其余保持不变，对应的mask向量为填0处为0，其余为1

​				  sequence_mask: 防止leak，因为主要是针对decoder，mask矩阵为上三角为0，对角线以及下三角为1

​					decoder_mask:  padding_mask与sequence_mask的乘积

​					*拓展问题：attention的mask，bert的mask？*

6、L2R（Learning To Rank）：

​					1、ndcg指标

​					2、point-wise，pair-wise，list-wise

​					(ps:这一块我不是太熟，所以请大家自己作答吧)

个人感想：推荐算法工程师其实相对而言涉猎更广，召回、排序、检索、模型、线上服务优化等等，这些也会在被或多或少的问到，大家如果觉得自己工程能力还行，可以考虑转推荐架构，算法技能可以用来锦上添花。如果对算法有热情，那么夯实基础很重要，掌握前沿的理论和trick也很重要。总之面试考察的也真的是越来越全面了，跑一个xgb就sp的时代早就过去了，祝各位好运。

​				 

